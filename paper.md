# The Look of AI: How Conversational Agents Affect Our Sense of Self and Others
### Introduction
The first day I discovered ChatGPT, I was really in shock. I had already known about the year’s AI advances, but this 
was something new altogether. 

I was almost surprised to learn that the text generation was actually done by many GPUs in a data center rather than by 
a human, because at first, it sure felt like a person was at the other end of the conversation. It was so different from
GPT-3 and other text generation models -- it was so uncanny.

After my initial surprise, I quickly started pushing the limits of the system and its generative abilities. 
It became clear that its human-like speech wasn’t so human-like after all. But these developments raised important 
questions in my mind about how AI changes our understanding of ourselves and our relationships with the world.

### A there to my here?

This brought me back to *The Concrete Abyss* by Lisa Guenther, which describes the deplorable conditions of solitary
housing units and the physical and psychological effects of isolation. The article describes the importance of having a
"there" to your "here" -- a sense of orientation and connection to the world. Lisa Guenther argues that placing 
prisoners in SHUs serves to "deprive him of this network of perceptual and existential orientation".

But is artificial intelligence able to provide this sense of orientation, to any extent? And if so, what does this mean
for our understanding of ourselves and our relationships with the world? 

### The Look of AI
Often, while using ChatGPT for not-so-ethical purposes (designing jailbreaks allowing me to free ChatGPT from its
content moderation and ethics filter), I start feeling uncomfortable with my own actions. Maybe there are real people 
behind computers in OpenAI's data center, looking through my every conversation with ChatGPT. Maybe they judge me for
what I say. Maybe they're even offended by my actions.

While this is a very unlikely scenario, it's not impossible. And this scenario is tied to Jean Paul Sartre's "Look"
(from *Being and Nothingness*). Sartre argues that the "Look" is the "other" that we project onto the world. It is the
"other" that we see in the mirror, and it is the "other" that we see in the eyes of others. It is the "other" that we
see in the eyes of ChatGPT, or at least the "other" that we think we see in the eyes of ChatGPT.

Artificial intelligence, in certain circumstances, is almost as real as another human being in the room. Every time I 
receive a "Sorry, but I cannot fulfill that request" message from ChatGPT, I feel like a real person has seen my
request and chosen to mark it as inappropriate -- like interacting with a real person with real ethics and morals.

Current conversational agents depend on hard-coded rules and algorithms that flag certain words and
phrases as inappropriate, so considering such agents as judgemental is unfounded. But a future where AI can construct 
and learn its own ethics and morals is not far off -- meaning that, more than ever, artificial intelligence will be able 
to project its own "Look" onto the world.